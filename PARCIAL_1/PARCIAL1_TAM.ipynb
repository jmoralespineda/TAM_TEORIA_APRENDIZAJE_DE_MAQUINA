{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyG1ILSfZjTmlevOUCn07r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmoralespineda/TAM_TEORIA_APRENDIZAJE_DE_MAQUINA/blob/main/PARCIAL_1/PARCIAL1_TAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Integrantes: Valeria Garcia - Diego Salazar - Julian David Morales**"
      ],
      "metadata": {
        "id": "Qfvm3jcec8lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Link de ejercicios**\n",
        "Link: https://github.com/jmoralespineda/TAM_TEORIA_APRENDIZAJE_DE_MAQUINA/blob/main/PARCIAL_1/Punto1_ParcialTAM.pdf"
      ],
      "metadata": {
        "id": "ykpOJbDTdBBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Cuadro comparativo**"
      ],
      "metadata": {
        "id": "7nwy9MsJdRc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Regresor | Modelo matemático | función de costo | Estrategia de optimización | Relación con esquemas de regresion discutidos en el punto 1 | Escalabilidad |\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "|LinearRegresor| Utiliza un modelo lineal con coeficientes $w=\\{w_{1},w_{2},...,w_{p}\\}$ que minimiza la suma de los minimos cuadrados entre los valores reales $y_{i}$ y los valores predichos $\\hat{y}_{i}$ | Forma lineal: $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2$ en forma matricial: $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ |Mínimos cuadrados ordinarios (OLS) | El LinearRegressor representa la forma más simple del modelo lineal Gaussiano, y todos los demás métodos del punto 1 son generalizaciones de él que: $\\bull$ ajustan la función de costo (añadiendo regularización o términos probabilísticos), $\\bull$ cambian el espacio de representación (introduciendo kernels),o transforman el modelo de determinista → probabilístico (Bayesiano, GP).|Buena para datos pequeños/medianos, limitada para grandes volúmenes|\n",
        "| Lasso | Regresión lineal con **regularización L1**, que fuerza algunos coeficientes a ser exactamente cero (selección de variables). | $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - X_i \\boldsymbol{\\beta})^2 + \\boldsymbol{alpha} \\boldsymbol{\\beta} $ | Descenso por coordenadas (*Coordinate Descent*). | Lasso está directamente relacionado con los modelos de la pregunta 1 porque: $\\bull$ Surge del mismo modelo de regresión lineal con ruido gaussiano. $\\bull$ Es una variación del esquema MAP, con un prior Laplaciano en lugar de Gaussiano. $\\bull$ Comparte la misma base matemática que Ridge y Bayesian linear regression, pero: usa una norma L1 en lugar de L2 y produce modelos dispersos (con pesos nulos). | Alta para datos medianos; eficiente en modelos dispersos (muchas variables irrelevantes). |\n",
        "| ElasticNet | Regresión lineal con **combinación de penalizaciones L1 y L2**, equilibrando sparsity y estabilidad numérica. | $$ J(\\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_{i=1}^{n} (y_{i} - X_{i} \\boldsymbol{\\beta})^2 $$+ $$\\alpha [ \\rho \\| \\boldsymbol{\\beta}_{1} \\|$$ + $$ (1- \\rho) \\| \\beta \\|_{2}^2$$| Coordinate Descent | La regresión Elastic Net se relaciona directamente con los esquemas de regresión que viste en la Pregunta 1, en especial con mínimos cuadrados regularizados (Ridge) y con el modelo LASSO | Esaclabilidad Alta, combina estabilidad de Ridge y sparsity de Lasso |\n",
        "|KernelRidge | Extiende la regresión Ridge a un **espacio de características no lineal** mediante funciones kernel. |$J(\\alpha)=\\| y - K \\alpha \\|^2 + \\lambda \\alpha^{T} K \\alpha $ | Solución analítica $(K + \\lambda I)\\boldsymbol{\\alpha} = \\mathbf{y}$ | Kernel Ridge parte del modelo lineal (OLS), añade regularización (Ridge/MAP), incorpora una base probabilística (MLE/MAP/Bayesiano) y se generaliza al espacio de funciones (GP). | Escalabilidad Limitada requiere invertir matrices $O(n^3)$, es poco escalable a grandes datasets |\n",
        "|SGDRegressor | Regresión lineal entrenada mediante **gradiente descendente estocástico**, actualizando los parámetros con muestras o mini-lotes. | $J(\\beta) = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - X_i\\beta)^2 + \\alpha R(\\beta)$ con R regularización L1/L2 | Gradiente descendente estocástico (*SGD*).|El SGDRegressor no define un nuevo modelo estadístico, sino una estrategia de optimización iterativa para los mismos esquemas lineales discutidos en la pregunta 1. Según la función de pérdida y el tipo de regularización elegidos, puede emular OLS, Ridge, Lasso o ElasticNet, pero sin la interpretación probabilística de los modelos bayesianos ni la flexibilidad no paramétrica de los modelos kernel o de procesos gaussianos. | Escalabilidad Excelente, adecuado para Big Data y aprendizaje en línea. |\n",
        "|BayesianRidge | Regresión lineal **bayesiana**, donde los coeficientes tienen distribuciones a priori normales, produce estimaciones probabilísticas. $\\hat{y} = X\\boldsymbol{\\beta}, \\quad \\boldsymbol{\\beta} \\sim \\mathcal{N}(0, \\lambda^{-1} I)$ | Log-verosimilitud negativa con regularización bayesiana |  Estimación iterativa tipo evidence maximization (EM) | La regresión Bayesiana Ridge (BayesianRidge) es una generalización probabilística del modelo de mínimos cuadrados regularizados (Ridge), que a su vez está estrechamente relacionada con los modelos MLE, MAP y el modelo Bayesiano lineal Gaussiano | Escalabilidad media, escalable a datos moderados |\n",
        "|Gaussian Process Regressor | Modelo **no paramétrico** que define una distribución sobre funciones posibles, usando un kernel para medir similitud entre puntos. | $J(\\theta) = \\frac{1}{2}\\mathbf{y}^\\top K_{\\theta}^{-1}\\mathbf{y} + \\frac{1}{2} \\log K_{\\theta} + \\frac{n}{2} \\log(2\\pi)$ |Ajuste de hiperparámetros por gradiente descendente. | El Gaussian Process Regressor unifica y amplía todos los modelos de regresión lineal y kernel vistos, el GPR es el marco bayesiano no paramétrico que engloba todos los esquemas de regresión de la pregunta 1. | Escalabilidad Muy limitada, \\(O(n^3)\\) en tiempo y \\(O(n^2)\\) en memoria. |\n",
        "|Support Vector Machines Regressor |Regresor **basado en márgenes**, que busca una función dentro de una tolerancia ε y usa vectores soporte para definir la frontera. |$J = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} L_\\varepsilon(y_i, \\hat{y}_i)$ | Programación cuadrática convexa (QP). | El SVR es una regresión lineal regularizada como Ridge, pero con una pérdida diferente (ε–insensible) y formulada en el espacio de kernels, lo que lo conecta directamente con los modelos kernelizados y bayesianos de la pregunta 1, aunque desde una perspectiva determinista y geométrica, no probabilística.|Escalabilidad Limitada, útil para datasets medianos ($<10⁴$ muestras). |\n",
        "|Random Forest Regressor|**Ensamble de árboles de decisión** entrenados sobre subconjuntos aleatorios del dataset (bagging).| $J = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2$|Crecimiento independiente de árboles y promediado de predicciones. |El Random Forest ocupa, dentro del marco general de la pregunta 1, un lugar análogo a los modelos Bayesianos y Kernel: realiza un suavizado adaptativo, no lineal y no paramétrico, sin necesidad de suposiciones explícitas sobre la forma de la función ni sobre distribuciones gaussianas. | Escalabilidad Muy alta, paralelizable y eficiente en grandes volúmenes de datos. |\n",
        "|Gradient Boosting Regressor|**Ensamble secuencial de árboles**, donde cada nuevo árbol corrige los errores del modelo previo.|$J = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i)$ |Descenso de gradiente sobre funciones (boosting). | El Gradient Boosting Regressor (GBR) en lugar de ser un modelo paramétrico lineal o bayesiano, es un método de ensamble secuencial basado en optimización funcional. Aun así, las conexiones teóricas con los modelos del punto 1 son claras si se mira desde el punto de vista de la minimización de una función de costo. | Esacalabilidad Alta, aunque más lenta que Random Forest, es más precisa. |\n",
        "|XGBoost|Versión optimizada de Gradient Boosting que incluye **regularización explícita y paralelización**.|$J = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(f_m)$|Boosting por gradiente de segundo orden con poda (*tree pruning*) y paralelización.  | XGBoost y los esquemas de regresión de la pregunta 1 persiguen el mismo objetivo fundamental: encontrar una función $f(x)$ que aproxime la relación entre los datos $x$ y las salidas $t$ minimizando un error esperado. Sin embargo, difieren en la forma de representar esa función y en cómo optimizan sus parámetros. | Escalabilidad Muy alta, diseñado para Big Data y GPU computing.|"
      ],
      "metadata": {
        "id": "VzOI9fC7dZU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Librería RAPIDS y comparación de regresores con implementaciones GPU**\n",
        "\n",
        "**¿Qué es RAPIDS?**\n",
        "\n",
        "**RAPIDS** es un conjunto de bibliotecas de código abierto desarrolladas por **NVIDIA** que permite ejecutar todo el flujo de *Data Science* y *Machine Learning* completamente en **GPU**, sin necesidad de reescribir el código tradicional de *scikit-learn* o *pandas*.\n",
        "\n",
        "### Componentes principales de RAPIDS\n",
        "\n",
        "- **cuDF** → equivalente a `pandas`, pero procesando *dataframes* en GPU\n",
        "- **cuML** → biblioteca de *machine learning* similar a `scikit-learn`, acelerada con CUDA\n",
        "- **cuGraph** → algoritmos de grafos en GPU\n",
        "- **cuXfilter**, **cuSpatial** → para visualización, geodatos y streaming\n",
        "\n",
        "### Objetivo principal\n",
        "\n",
        "**Acelerar el procesamiento de grandes volúmenes de datos** (especialmente cuando `N` o `P` son grandes) mediante cómputo paralelo sobre CUDA. Gracias a que mantiene la **misma interfaz que `scikit-learn`**, es posible migrar código CPU → GPU con cambios mínimos.\n",
        "\n",
        "### Ventajas principales\n",
        "\n",
        "- Aceleración significativa en tareas de regresión, clasificación y clustering\n",
        "- API compatible con `scikit-learn`\n",
        "- Ejecución completamente en GPU (sin depender del CPU)\n",
        "\n",
        "### Limitaciones\n",
        "\n",
        "- Algunos algoritmos no están aún disponibles o son parciales (p. ej. `GaussianProcessRegressor`)\n",
        "- Las operaciones con datos pequeños pueden no mostrar mejora (debido al overhead de transferencia CPU↔GPU)\n",
        "- Ciertas funciones (como kernels precomputados o datos *sparse*) pueden forzar una ejecución en CPU\n",
        "\n",
        "---\n",
        "\n",
        "## Cómo migrar un pipeline sklearn → RAPIDS (paso a paso, con código)\n",
        "\n",
        "**Contexto:** quieres acelerar un pipeline con `pandas` + `sklearn`. Supongamos un regresor `LinearRegression` o `RandomForestRegressor`.\n",
        "\n",
        "### 1) Instalar y comprobar entorno\n",
        "\n",
        "- Instalar RAPIDS compatible con tu CUDA (recomendado: usar conda o RAPIDS con Docker)\n",
        "- Verificar GPU disponible usando `nvidia-smi` y compatibilidad de versiones\n",
        "- Documentación: [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "### 2) Convertir pandas → cudf\n",
        "\n",
        "Primero importamos las librerías necesarias y convertimos el DataFrame de pandas a cuDF:\n",
        "\n",
        "    import cudf\n",
        "    import pandas as pd\n",
        "    \n",
        "    # df: pandas DataFrame\n",
        "    gdf = cudf.from_pandas(df)        # copia a GPU (memcpy)\n",
        "    # o si lees parquet/csv con cuDF: cudf.read_csv(...)\n",
        "\n",
        "### 3) Entrenar un modelo cuML (ejemplo: LinearRegression)\n",
        "\n",
        "Una vez que los datos están en la GPU, entrenamos el modelo directamente:\n",
        "\n",
        "    from cuml.linear_model import LinearRegression\n",
        "    \n",
        "    X = gdf[feature_cols]   # cudf.DataFrame\n",
        "    y = gdf[target_col]     # cudf.Series\n",
        "    \n",
        "    model = LinearRegression(fit_intercept=True, algorithm='svd')\n",
        "    model.fit(X, y)\n",
        "    preds = model.predict(X_test)   # devuelve cudf.Series o cupy array\n",
        "\n",
        "**Observaciones:** algunos modelos aceptan `cupy`/`numpy` arrays; la salida puede ser `cudf` o `cupy`. Ver [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "### 4) RandomForest (ejemplo)\n",
        "\n",
        "Para modelos basados en árboles como Random Forest:\n",
        "\n",
        "    from cuml.ensemble import RandomForestRegressor\n",
        "    \n",
        "    rf = RandomForestRegressor(n_estimators=100, max_depth=16)\n",
        "    rf.fit(X, y)\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "Para ensembles grandes puedes usar Dask-cuML para multi-GPU. Ver [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "\n",
        "### 5) Evaluation (volver a CPU si quieres usar scikit metrics)\n",
        "\n",
        "Si necesitas usar métricas de scikit-learn, puedes transferir los resultados de vuelta a CPU:\n",
        "\n",
        "    import pandas as pd\n",
        "    y_pred_cpu = y_pred.to_pandas()   # transfer back to host if needed\n",
        "\n",
        "**Evita transferencias repetidas:** calcula métricas en GPU cuando sea posible (cuML / cuDF algunas métricas disponibles).\n",
        "\n",
        "---\n",
        "\n",
        "## Multi-GPU y escalado con Dask\n",
        "\n",
        "- **Dask-cuDF:** particiona `cudf` en múltiples GPUs (o nodos); Dask coordina tareas. Es la forma recomendada para datasets que no caben en la memoria de una sola GPU. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- **cuML + Dask:** muchos estimators tienen versiones `cuml.dask.*` que permiten entrenamiento distribuido (RandomForest, KNN, etc.). La integración exige configurar un `Dask` cluster con workers que tengan GPUs asignadas. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "**Consejo:** usar Dask tiene coste de orquestación; para muchos problemas una sola GPU potente puede bastar. Para multi-GPU, planifica particionado de datos y comunicación (PCIe / NVLink) para minimizar overhead.\n",
        "\n",
        "---\n",
        "\n",
        "## Ventajas y limitaciones prácticas (cuando usar / evitar)\n",
        "\n",
        "### ✅ Ventajas\n",
        "\n",
        "- Aceleración significativa en ETL y algunos modelos (RF, Linear, KNN, clustering). [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- API familiar → curva de aprendizaje corta. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- Buenas integraciones con XGBoost GPU y Dask para escalado. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "\n",
        "### ⚠️ Limitaciones / riesgos\n",
        "\n",
        "- **Overhead de copia CPU↔GPU:** para datasets pequeños la ganancia puede ser nula o negativa. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- **Memoria GPU limitada:** si tus datos no caben, necesitas Dask o streaming. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- Algunas implementaciones aún son experimentales y la API evoluciona (revisa la versión). [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- No todos los algoritmos avanzados (p. ej. GP completos) están optimizados en cuML; en esos casos usar librerías específicas GPU o aproximaciones. (GP → ver alternativas)\n",
        "\n",
        "---\n",
        "\n",
        "## Buenas prácticas y checklist antes de migrar a RAPIDS\n",
        "\n",
        "1. **Perfilado:** medir tiempos actuales CPU (ETL + training) para tener baseline\n",
        "2. **Comprobar que los datos caben en GPU** (o diseñar particionado)\n",
        "3. **Convertir a cudf al inicio** y hacer el mayor número de operaciones en GPU\n",
        "4. **Evitar idas y vueltas:** minimizar `.to_pandas()`/`.to_numpy()`\n",
        "5. **Usar Dask-cuDF** para escalado multi-GPU, y probar con datos reales. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "6. **Version pinning:** documentar versión RAPIDS/CUDA usada — cambios en la API son frecuentes\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla comparativa: Regresores de scikit-learn y sus equivalentes en RAPIDS (cuML)\n",
        "\n",
        "| **Modelo / Regresor** | **Scikit-learn** | **Implementación en RAPIDS (cuML)** | **Hiperparámetros principales** | **Notas / Diferencias** |\n",
        "|------------------------|------------------|-------------------------------------|---------------------------------|--------------------------|\n",
        "| **Linear Regression** | `sklearn.linear_model.LinearRegression` | `cuml.LinearRegression` | `fit_intercept`, `normalize`, método (`eig`, `svd`, `qr`) | Implementación GPU con solvers rápidos y paralelizados |\n",
        "| **Ridge Regression (L2)** | `sklearn.linear_model.Ridge` | `cuml.Ridge` | `alpha`, `solver`, `fit_intercept` | Equivalente a MAP con prior gaussiano; usa Eigen o SVD |\n",
        "| **Lasso / ElasticNet** | `sklearn.linear_model.Lasso`, `ElasticNet` | `cuml.ElasticNet` | `alpha`, `l1_ratio`, `max_iter`, `tol` | Lasso puede aproximarse vía `ElasticNet` con `l1_ratio=1.0` |\n",
        "| **SGD Regressor** | `sklearn.linear_model.SGDRegressor` | `cuml.SGD` (experimental) | `loss`, `penalty`, `alpha`, `max_iter`, `learning_rate` | Implementación parcial; puede caer a CPU en casos específicos |\n",
        "| **Bayesian Ridge** | `sklearn.linear_model.BayesianRidge` | *(no disponible completo en cuML)* | `alpha_1`, `alpha_2`, `lambda_1`, `lambda_2` | No implementado aún con inferencia bayesiana en GPU |\n",
        "| **Kernel Ridge** | `sklearn.kernel_ridge.KernelRidge` | `cuml.KernelRidge` | `alpha`, `kernel`, `gamma`, `degree`, `coef0` | Soporta kernels RBF, linear y poly. No acepta matrices *sparse* |\n",
        "| **Support Vector Regression (SVR)** | `sklearn.svm.SVR` | `cuml.svm.SVR` | `C`, `epsilon`, `kernel`, `gamma`, `degree` | Soporte GPU para kernels comunes; kernels precomputados no |\n",
        "| **Random Forest Regressor** | `sklearn.ensemble.RandomForestRegressor` | `cuml.ensemble.RandomForestRegressor` | `n_estimators`, `max_depth`, `min_samples_split`, `max_features`, `bootstrap` | Totalmente en GPU. Muy superior en datasets grandes |\n",
        "| **Gradient Boosting / XGBoost** | `sklearn.ensemble.GradientBoostingRegressor`, `xgboost.XGBRegressor` | `cuml.GradientBoostingRegressor`, `xgboost` (con GPU) | `n_estimators`, `learning_rate`, `max_depth`, `subsample` | XGBoost GPU es altamente optimizado; integrado con cuDF/cuML |\n",
        "| **Gaussian Process Regressor** | `sklearn.gaussian_process.GaussianProcessRegressor` | *(no implementado en cuML)* | `kernel`, `alpha`, `optimizer`, `normalize_y` | Computacionalmente costoso en GPU; aún sin soporte estable |\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "RAPIDS facilita trasladar los flujos de trabajo clásicos de *scikit-learn* a la GPU manteniendo casi la misma sintaxis. Las bibliotecas más importantes son **cuDF** (para manipular datos) y **cuML** (para ejecutar modelos de regresión).\n",
        "\n",
        "Los modelos basados en árboles (RandomForest, XGBoost) y lineales (Linear, Ridge, ElasticNet, SVR) tienen implementaciones maduras y estables en GPU, ofreciendo **aceleraciones entre 10× y 50×** en comparación con CPU en conjuntos de datos medianos o grandes.\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias bibliográficas\n",
        "\n",
        "- NVIDIA Corporation. *RAPIDS: Open GPU Data Science Framework*. Disponible en: https://rapids.ai\n",
        "- NVIDIA Developer Blog. *Accelerating Machine Learning with cuML and RAPIDS*. Disponible en: https://developer.nvidia.com/blog/tag/rapids/\n",
        "- RAPIDS cuML Documentation. *cuML API Reference (v25.06)*. Disponible en: https://docs.rapids.ai/api/cuml/stable/\n",
        "- Scikit-Learn Documentation. *Supervised Learning Models*. Disponible en: https://scikit-learn.org/stable/\n",
        "- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer\n",
        "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer"
      ],
      "metadata": {
        "id": "Jp0JSc17dsQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Conjunto de datos nfl-big-data-bowl-2026-prediction**\n",
        "**1. Objetivo Principal del Concurso NFL Big Data Bowl 2026 - Prediction 🏈**\n",
        "El objetivo de la competencia \"NFL Big Data Bowl 2026 - Prediction\" es predecir el movimiento de los jugadores en el campo durante una jugada de pase, específicamente en los fotogramas que ocurren después de que el mariscal de campo (quarterback) lanza el balón.\n",
        "\n",
        "A los participantes se les proporciona una gran cantidad de datos de seguimiento de jugadores (Next Gen Stats) antes del lanzamiento. El desafío consiste en construir un modelo de regresión que pueda pronosticar con precisión las coordenadas (x, y) de cada jugador en el campo para cada fotograma (10 fotogramas por segundo) mientras el balón está en el aire. En esencia, es un problema de predicción de trayectorias espacio-temporales.\n",
        "\n",
        "**2. Descripción de las Variables**\n",
        "El conjunto de datos es rico y se compone de varios archivos que deben ser unidos. Las variables clave se pueden agrupar de la siguiente manera:\n",
        "\n",
        "Variables de Entrada (Features)\n",
        "Estas variables describen el estado del juego y de los jugadores antes del lanzamiento del balón.\n",
        "\n",
        "Variables de Salida (Target)\n",
        "El objetivo es predecir la trayectoria futura de cada jugador.\n",
        "\n",
        "|Variables de entrada|Descripción|Tipo de variable|Variables de Salida| Descripción|Tipo de variable|\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "|`game_id`|Identificador del partido|único (numérico)|`game_id`|Identificador del partido|único (numérico)|\n",
        "|`play_id`| Identificador de la jugada|no único entre juegos (numérico)|`play_id`|Identificador de la jugada|no único entre juegos (numérico)|\n",
        "|`player_to_predict`|si la predicción x/y para este jugador se puntuará o no|(booleana)||||\n",
        "|`nfl_id`| Identificador para cada jugador.|único entre los jugadores (numérico)|`nfl_id`| Identificador para cada jugador.|único entre los jugadores (numérico)|\n",
        "|`frame_id`|Identificador de fotograma para cada reproducción/tipo, comenzando por 1 para cada tipo de `game_id` archivo play_id(entrada o salida). El valor máximo para un determinado game_idserá `play_id` el `nfl_id` mismo que el num_frames_outputvalor del archivo de entrada correspondiente.|(numérico)|`frame_id`|Identificador de fotograma para cada reproducción/tipo, comenzando por 1 para cada tipo de `game_id` archivo play_id(entrada o salida). El valor máximo para un determinado game_idserá `play_id` el `nfl_id` mismo que el num_frames_outputvalor del archivo de entrada correspondiente.|(numérico)|\n",
        "|`play_direction`|Dirección en la que se mueve la ofensiva |(izquierda o derecha)||||\n",
        "|`absolute_yardline_number`| Distancia desde la zona de anotación para el equipo que posee el balón|(numérica)||||\n",
        "|`player_name`|nombre del jugador | texto ||||\n",
        "|`player_height`|altura del jugador| pies-pulgadas|||||\n",
        "|`player_weight`|peso del jugador| libras||||\n",
        "|`player_birth_date`|fecha de nacimiento|aaaa-mm-dd||||\n",
        "|`player_position`|la posición del jugador (el rol específico en el campo que normalmente desempeña)||||\n",
        "|`player_side`|el jugador del equipo está en (ataque o defensa)|(ataque o defensa)||||\n",
        "|`player_role`|rol que tiene el jugador en juego| (cobertura defensiva, receptor objetivo, pasador u otro corredor de ruta)||||\n",
        "|`x`|Posición del jugador a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas.|numérico|`x`|Posición del jugador a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas.|numérico|\n",
        "|`y`|Posición del jugador a lo largo del eje largo del campo, generalmente entre 0 y 53,3 yardas.|numérico|`y`|Posición del jugador a lo largo del eje largo del campo, generalmente entre 0 y 53,3 yardas.|numérico|\n",
        "|`s`|Velocidad en yardas/segundo|numérica|||||\n",
        "|`a`|Aceleración en yardas/segundos^2|numérica|||||\n",
        "|`o`|orientación del jugador|grados (0-360)|||||\n",
        "|`dir`|angulo de movimiento del jugador|grados (0-360)|||||\n",
        "|`num_frames_output`|numero de fotogramas a predecir en los datos de salida para el `game_id` dado `play_id` y `nfl_id`|numérico|||||\n",
        "|`ball_land_x`|Posición de aterrizaje de la pelota a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas|numérico||||"
      ],
      "metadata": {
        "id": "Ogn8cxNud4DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Análisis Exploratorio Básico (EDA)**\n",
        "Un buen EDA es fundamental aquí para entender la dinámica del juego. Los pasos clave serían:\n",
        "\n",
        "- Visualización de Jugadas: Crear animaciones de jugadas individuales. Esto es crucial para desarrollar una intuición sobre los patrones de movimiento. Se puede trazar la posición (x, y) de cada jugador a lo largo del tiempo, coloreando por equipo y destacando al pasador.\n",
        "\n",
        "- Distribución de Variables Físicas: Analizar las distribuciones de velocidad (s), aceleración (a), y orientación (o) por posición del jugador (player_position). Se esperaría que los Receptores Abiertos (WR) alcancen velocidades más altas que los Linieros Ofensivos (OL).\n",
        "\n",
        "- Análisis de Rutas: Graficar las trayectorias de los receptores para identificar patrones de rutas comunes (ej. \"slant\", \"go\", \"post\").\n",
        "\n",
        "- Correlaciones: Investigar las relaciones entre las variables de entrada. Por ejemplo, la aceleración debería estar correlacionada con los cambios en la velocidad.\n",
        "\n",
        "Análisis de Datos Faltantes: Identificar qué columnas tienen valores nulos y en qué proporción. Es común encontrar faltantes en variables contextuales de la jugada.\n",
        "\n",
        "**Codificación de Valores Faltantes y Variables Categóricas**\n",
        "Valores Faltantes:\n",
        "\n",
        "Para variables numéricas como o (orientación) o dir (dirección), una estrategia podría ser la imputación con la media o la mediana del jugador o de su grupo de posición.\n",
        "\n",
        "Otra técnica podría ser la propagación hacia adelante o hacia atrás (ffill/bfill) dentro de una misma jugada, asumiendo que el movimiento no cambia drásticamente en un fotograma.\n",
        "\n",
        "**Variables Categóricas:**\n",
        "\n",
        "player_position, player_role, player_side: Estas son variables categóricas nominales. La mejor estrategia es el One-Hot Encoding. Esto crea columnas binarias para cada categoría, evitando que el modelo asuma un orden inexistente.\n",
        "\n",
        "player_height: Esta variable viene como texto (ej. \"6-2\"). Se debe convertir a una variable numérica (en pulgadas o centímetros) antes de usarla.\n",
        "\n",
        "**Estrategias de Ingeniería de Características (Feature Engineering)**\n",
        "Aquí es donde se puede obtener la mayor ventaja competitiva. El objetivo es crear variables que capturen la compleja interacción entre jugadores. 🧠\n",
        "\n",
        "- Estandarización del Campo: Las jugadas ocurren en diferentes direcciones. Un paso crucial es estandarizar la dirección de la jugada para que la ofensiva siempre ataque hacia el mismo lado del campo. Esto hace que los patrones sean comparables entre jugadas.\n",
        "\n",
        "- Características Relativas al Balón: Calcular la distancia y el ángulo de cada jugador con respecto al balón en cada fotograma.\n",
        "\n",
        "- Características Inter-Jugadores:\n",
        "\n",
        "- Distancia al Jugador Más Cercano: Para cada jugador, calcular la distancia a su defensor más cercano (si es ofensivo) o al jugador que está cubriendo (si es defensivo).\n",
        "\n",
        "- Distancia al Quarterback: Especialmente relevante para los pass rushers y la línea ofensiva que los bloquea.\n",
        "\n",
        "- Características Cinemáticas Derivadas:\n",
        "\n",
        "- Componentes de Velocidad: Descomponer la velocidad s en sus componentes vx y vy usando la dirección dir. Esto es más útil para el modelo que la velocidad escalar.\n",
        "\n",
        "vx = s * cos(dir)\n",
        "\n",
        "vy = s * sin(dir)\n",
        "\n",
        "- Aceleración Angular: Calcular la tasa de cambio de la orientación o para medir qué tan rápido está girando un jugador.\n",
        "\n",
        "**Características de Series Temporales:**\n",
        "\n",
        "Para cada jugador, crear características que resuman su movimiento en los últimos k fotogramas, como velocidad promedio, aceleración máxima, o cambio total en la dirección.\n",
        "\n",
        "Al implementar estas estrategias, el conjunto de datos se enriquece enormemente, pasando de ser una simple colección de coordenadas a un sistema que describe las complejas relaciones e interacciones en el campo de juego. Esto es fundamental para que los modelos de regresión puedan aprender los patrones de movimiento de manera efectiva."
      ],
      "metadata": {
        "id": "ugQuvZxUePSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%markdown\n",
        "\n",
        "# Métricas de Evaluación de Modelos de Regresión\n",
        "\n",
        "## Notación\n",
        "\n",
        "- **yᵢ**: El valor real u observado para la i-ésima observación.\n",
        "- **ŷᵢ**: El valor predicho por el modelo para la i-ésima observación.\n",
        "- **N**: El número total de observaciones en el conjunto de datos.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. MAE (Mean Absolute Error / Error Absoluto Medio)\n",
        "\n",
        "El MAE es quizás la métrica de error más intuitiva y fácil de interpretar.\n",
        "\n",
        "### ¿Qué Mide?\n",
        "Mide el promedio de las diferencias absolutas entre los valores reales y los predichos. En palabras simples, te dice: \"en promedio, ¿por cuántas unidades me equivoqué, sin importar si fue por arriba o por abajo?\"\n",
        "\n",
        "### Cálculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error**: Para cada punto, calcula la diferencia entre el valor real y el predicho (yᵢ - ŷᵢ).\n",
        "2. **Obtener el Valor Absoluto**: Convierte cada error a un valor positivo tomando su valor absoluto (|yᵢ - ŷᵢ|). Esto elimina la dirección del error.\n",
        "3. **Sumar los Errores**: Suma todos los errores absolutos que calculaste.\n",
        "4. **Calcular el Promedio**: Divide la suma total por el número de observaciones (N).\n",
        "\n",
        "### Fórmula Matemática:\n",
        "\n",
        "$$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
        "\n",
        "### Interpretación:\n",
        "\n",
        "- **Unidades**: El resultado está en las mismas unidades que tu variable objetivo. Si estás prediciendo yardas, el MAE estará en yardas.\n",
        "- **Escala**: Un valor de 0 es un modelo perfecto. Valores más altos indican un mayor error promedio.\n",
        "- **Fortaleza**: Es menos sensible a los valores atípicos (outliers) que el MSE, ya que no eleva los errores al cuadrado.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. MSE (Mean Squared Error / Error Cuadrático Medio)\n",
        "\n",
        "El MSE es una de las métricas más comunes, especialmente en la optimización de modelos.\n",
        "\n",
        "### ¿Qué Mide?\n",
        "Mide el promedio de los errores al cuadrado. Su principal característica es que penaliza los errores grandes mucho más que los errores pequeños. Un error de 10 unidades contribuye 100 veces más al error total que un error de 1 unidad.\n",
        "\n",
        "### Cálculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error**: Para cada punto, encuentra la diferencia (yᵢ - ŷᵢ).\n",
        "2. **Elevar al Cuadrado**: Eleva cada error al cuadrado ((yᵢ - ŷᵢ)²). Esto convierte todos los errores en positivos y magnifica los más grandes.\n",
        "3. **Sumar los Errores**: Suma todos los errores cuadráticos.\n",
        "4. **Calcular el Promedio**: Divide la suma total por el número de observaciones (N).\n",
        "\n",
        "### Fórmula Matemática:\n",
        "\n",
        "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "### Interpretación:\n",
        "\n",
        "- **Unidades**: Las unidades están al cuadrado con respecto a la variable objetivo (ej. yardas²), lo que hace que su interpretación directa sea menos intuitiva. Por esta razón, a menudo se usa su raíz cuadrada, el RMSE (Root Mean Squared Error), que vuelve a las unidades originales.\n",
        "- **Sensibilidad**: Es muy sensible a los outliers. Un solo punto muy mal predicho puede disparar el valor del MSE.\n",
        "- **Uso**: Es muy popular porque la función de error cuadrático es convexa y diferenciable, lo que facilita los cálculos matemáticos en los algoritmos de optimización (como el descenso de gradiente).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. R² (Coefficient of Determination / Coeficiente de Determinación)\n",
        "\n",
        "R² no es una medida de error, sino una medida de qué tan bien se ajusta el modelo a los datos.\n",
        "\n",
        "### ¿Qué Mide?\n",
        "Mide la proporción de la varianza en la variable objetivo que es explicada por el modelo. Responde a la pregunta: \"¿qué porcentaje de la variabilidad de los datos es capturado por mi modelo, en comparación con un modelo muy simple que solo predice el promedio de los datos?\"\n",
        "\n",
        "### Cálculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error del Modelo (RSS)**: Calcula la suma de los errores al cuadrado de tu modelo. Esto es N×MSE. Se conoce como Suma de Cuadrados Residual (RSS).\n",
        "   \n",
        "   $$RSS = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "2. **Calcular el Error de un Modelo \"Base\" (TSS)**: Calcula la suma de los errores al cuadrado que obtendrías si tu predicción fuera siempre el promedio de todos los valores reales (ȳ). Esto representa la varianza total de los datos y se conoce como Suma de Cuadrados Total (TSS).\n",
        "   \n",
        "   $$TSS = \\sum_{i=1}^{N} (y_i - \\bar{y})^2$$\n",
        "\n",
        "3. **Calcular R²**: Compara el error de tu modelo con el error del modelo base.\n",
        "   \n",
        "   $$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
        "\n",
        "### Fórmula Matemática:\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}$$\n",
        "\n",
        "### Interpretación:\n",
        "\n",
        "- **Escala**: No tiene unidades. Su valor suele estar entre 0 y 1.\n",
        "- **R² = 1**: El modelo explica el 100% de la variabilidad de los datos. Es un ajuste perfecto.\n",
        "- **R² = 0**: El modelo no explica nada de la variabilidad. Es tan útil como simplemente predecir el promedio.\n",
        "- **R² < 0**: El modelo es peor que predecir el promedio. Esto indica un muy mal ajuste.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. MAPE (Mean Absolute Percentage Error / Error Porcentual Absoluto Medio)\n",
        "\n",
        "MAPE es muy popular en contextos de negocio porque expresa el error en términos de un porcentaje fácil de entender.\n",
        "\n",
        "### ¿Qué Mide?\n",
        "Mide el promedio del error absoluto, expresado como un porcentaje del valor real. Te dice: \"en promedio, ¿qué tan lejos estuvieron mis predicciones de los valores reales, en términos porcentuales?\"\n",
        "\n",
        "### Cálculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error Porcentual**: Para cada punto, calcula el error absoluto y divídelo por el valor real absoluto: $\\frac{|y_i - \\hat{y}_i|}{|y_i|}$.\n",
        "2. **Sumar los Errores Porcentuales**: Suma todos los errores porcentuales que calculaste.\n",
        "3. **Calcular el Promedio**: Divide la suma total por el número de observaciones (N).\n",
        "4. **Expresar como Porcentaje**: Multiplica el resultado por 100.\n",
        "\n",
        "### Fórmula Matemática:\n",
        "\n",
        "$$MAPE = \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - \\hat{y}_i|}{|y_i|}\\right) \\times 100\\%$$\n",
        "\n",
        "### Interpretación:\n",
        "\n",
        "- **Unidades**: Es un porcentaje (%). Un MAPE del 5% significa que, en promedio, el error de predicción es del 5% del valor real.\n",
        "- **Fortaleza**: Es muy fácil de interpretar y comunicar a audiencias no técnicas.\n",
        "- **Debilidad Crítica**: No se puede usar si los valores reales (yᵢ) son cero o muy cercanos a cero, ya que la división por cero es indefinida o resultaría en valores enormes, distorsionando el resultado final."
      ],
      "metadata": {
        "id": "4PSE2d8yeS-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Link desarrollo del parcial  - KAGGLE Base de datos NFL**\n",
        "lINK:https://www.kaggle.com/code/disalazarp/tam-parcial-1"
      ],
      "metadata": {
        "id": "vSLIT0sLemjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NZjyacnMeGFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZZZoG7DctPx"
      },
      "outputs": [],
      "source": [
        "Integrantes: Valeria Garcia - Diego Salazar - Julian David Morales"
      ]
    }
  ]
}