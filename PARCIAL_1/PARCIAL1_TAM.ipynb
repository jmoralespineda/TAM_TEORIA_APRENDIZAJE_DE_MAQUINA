{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyG1ILSfZjTmlevOUCn07r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmoralespineda/TAM_TEORIA_APRENDIZAJE_DE_MAQUINA/blob/main/PARCIAL_1/PARCIAL1_TAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Integrantes: Valeria Garcia - Diego Salazar - Julian David Morales**"
      ],
      "metadata": {
        "id": "Qfvm3jcec8lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Link de ejercicios**\n",
        "Link: https://github.com/jmoralespineda/TAM_TEORIA_APRENDIZAJE_DE_MAQUINA/blob/main/PARCIAL_1/Punto1_ParcialTAM.pdf"
      ],
      "metadata": {
        "id": "ykpOJbDTdBBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Cuadro comparativo**"
      ],
      "metadata": {
        "id": "7nwy9MsJdRc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Regresor | Modelo matem√°tico | funci√≥n de costo | Estrategia de optimizaci√≥n | Relaci√≥n con esquemas de regresion discutidos en el punto 1 | Escalabilidad |\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "|LinearRegresor| Utiliza un modelo lineal con coeficientes $w=\\{w_{1},w_{2},...,w_{p}\\}$ que minimiza la suma de los minimos cuadrados entre los valores reales $y_{i}$ y los valores predichos $\\hat{y}_{i}$ | Forma lineal: $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2$ en forma matricial: $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ |M√≠nimos cuadrados ordinarios (OLS) | El LinearRegressor representa la forma m√°s simple del modelo lineal Gaussiano, y todos los dem√°s m√©todos del punto 1 son generalizaciones de √©l que: $\\bull$ ajustan la funci√≥n de costo (a√±adiendo regularizaci√≥n o t√©rminos probabil√≠sticos), $\\bull$ cambian el espacio de representaci√≥n (introduciendo kernels),o transforman el modelo de determinista ‚Üí probabil√≠stico (Bayesiano, GP).|Buena para datos peque√±os/medianos, limitada para grandes vol√∫menes|\n",
        "| Lasso | Regresi√≥n lineal con **regularizaci√≥n L1**, que fuerza algunos coeficientes a ser exactamente cero (selecci√≥n de variables). | $J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - X_i \\boldsymbol{\\beta})^2 + \\boldsymbol{alpha} \\boldsymbol{\\beta} $ | Descenso por coordenadas (*Coordinate Descent*). | Lasso est√° directamente relacionado con los modelos de la pregunta 1 porque: $\\bull$ Surge del mismo modelo de regresi√≥n lineal con ruido gaussiano. $\\bull$ Es una variaci√≥n del esquema MAP, con un prior Laplaciano en lugar de Gaussiano. $\\bull$ Comparte la misma base matem√°tica que Ridge y Bayesian linear regression, pero: usa una norma L1 en lugar de L2 y produce modelos dispersos (con pesos nulos). | Alta para datos medianos; eficiente en modelos dispersos (muchas variables irrelevantes). |\n",
        "| ElasticNet | Regresi√≥n lineal con **combinaci√≥n de penalizaciones L1 y L2**, equilibrando sparsity y estabilidad num√©rica. | $$ J(\\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_{i=1}^{n} (y_{i} - X_{i} \\boldsymbol{\\beta})^2 $$+ $$\\alpha [ \\rho \\| \\boldsymbol{\\beta}_{1} \\|$$ + $$ (1- \\rho) \\| \\beta \\|_{2}^2$$| Coordinate Descent | La regresi√≥n Elastic Net se relaciona directamente con los esquemas de regresi√≥n que viste en la Pregunta 1, en especial con m√≠nimos cuadrados regularizados (Ridge) y con el modelo LASSO | Esaclabilidad Alta, combina estabilidad de Ridge y sparsity de Lasso |\n",
        "|KernelRidge | Extiende la regresi√≥n Ridge a un **espacio de caracter√≠sticas no lineal** mediante funciones kernel. |$J(\\alpha)=\\| y - K \\alpha \\|^2 + \\lambda \\alpha^{T} K \\alpha $ | Soluci√≥n anal√≠tica $(K + \\lambda I)\\boldsymbol{\\alpha} = \\mathbf{y}$ | Kernel Ridge parte del modelo lineal (OLS), a√±ade regularizaci√≥n (Ridge/MAP), incorpora una base probabil√≠stica (MLE/MAP/Bayesiano) y se generaliza al espacio de funciones (GP). | Escalabilidad Limitada requiere invertir matrices $O(n^3)$, es poco escalable a grandes datasets |\n",
        "|SGDRegressor | Regresi√≥n lineal entrenada mediante **gradiente descendente estoc√°stico**, actualizando los par√°metros con muestras o mini-lotes. | $J(\\beta) = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - X_i\\beta)^2 + \\alpha R(\\beta)$ con R regularizaci√≥n L1/L2 | Gradiente descendente estoc√°stico (*SGD*).|El SGDRegressor no define un nuevo modelo estad√≠stico, sino una estrategia de optimizaci√≥n iterativa para los mismos esquemas lineales discutidos en la pregunta 1. Seg√∫n la funci√≥n de p√©rdida y el tipo de regularizaci√≥n elegidos, puede emular OLS, Ridge, Lasso o ElasticNet, pero sin la interpretaci√≥n probabil√≠stica de los modelos bayesianos ni la flexibilidad no param√©trica de los modelos kernel o de procesos gaussianos. | Escalabilidad Excelente, adecuado para Big Data y aprendizaje en l√≠nea. |\n",
        "|BayesianRidge | Regresi√≥n lineal **bayesiana**, donde los coeficientes tienen distribuciones a priori normales, produce estimaciones probabil√≠sticas. $\\hat{y} = X\\boldsymbol{\\beta}, \\quad \\boldsymbol{\\beta} \\sim \\mathcal{N}(0, \\lambda^{-1} I)$ | Log-verosimilitud negativa con regularizaci√≥n bayesiana |  Estimaci√≥n iterativa tipo evidence maximization (EM) | La regresi√≥n Bayesiana Ridge (BayesianRidge) es una generalizaci√≥n probabil√≠stica del modelo de m√≠nimos cuadrados regularizados (Ridge), que a su vez est√° estrechamente relacionada con los modelos MLE, MAP y el modelo Bayesiano lineal Gaussiano | Escalabilidad media, escalable a datos moderados |\n",
        "|Gaussian Process Regressor | Modelo **no param√©trico** que define una distribuci√≥n sobre funciones posibles, usando un kernel para medir similitud entre puntos. | $J(\\theta) = \\frac{1}{2}\\mathbf{y}^\\top K_{\\theta}^{-1}\\mathbf{y} + \\frac{1}{2} \\log K_{\\theta} + \\frac{n}{2} \\log(2\\pi)$ |Ajuste de hiperpar√°metros por gradiente descendente. | El Gaussian Process Regressor unifica y ampl√≠a todos los modelos de regresi√≥n lineal y kernel vistos, el GPR es el marco bayesiano no param√©trico que engloba todos los esquemas de regresi√≥n de la pregunta 1. | Escalabilidad Muy limitada, \\(O(n^3)\\) en tiempo y \\(O(n^2)\\) en memoria. |\n",
        "|Support Vector Machines Regressor |Regresor **basado en m√°rgenes**, que busca una funci√≥n dentro de una tolerancia Œµ y usa vectores soporte para definir la frontera. |$J = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} L_\\varepsilon(y_i, \\hat{y}_i)$ | Programaci√≥n cuadr√°tica convexa (QP). | El SVR es una regresi√≥n lineal regularizada como Ridge, pero con una p√©rdida diferente (Œµ‚Äìinsensible) y formulada en el espacio de kernels, lo que lo conecta directamente con los modelos kernelizados y bayesianos de la pregunta 1, aunque desde una perspectiva determinista y geom√©trica, no probabil√≠stica.|Escalabilidad Limitada, √∫til para datasets medianos ($<10‚Å¥$ muestras). |\n",
        "|Random Forest Regressor|**Ensamble de √°rboles de decisi√≥n** entrenados sobre subconjuntos aleatorios del dataset (bagging).| $J = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2$|Crecimiento independiente de √°rboles y promediado de predicciones. |El Random Forest ocupa, dentro del marco general de la pregunta 1, un lugar an√°logo a los modelos Bayesianos y Kernel: realiza un suavizado adaptativo, no lineal y no param√©trico, sin necesidad de suposiciones expl√≠citas sobre la forma de la funci√≥n ni sobre distribuciones gaussianas. | Escalabilidad Muy alta, paralelizable y eficiente en grandes vol√∫menes de datos. |\n",
        "|Gradient Boosting Regressor|**Ensamble secuencial de √°rboles**, donde cada nuevo √°rbol corrige los errores del modelo previo.|$J = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i)$ |Descenso de gradiente sobre funciones (boosting). | El Gradient Boosting Regressor (GBR) en lugar de ser un modelo param√©trico lineal o bayesiano, es un m√©todo de ensamble secuencial basado en optimizaci√≥n funcional. Aun as√≠, las conexiones te√≥ricas con los modelos del punto 1 son claras si se mira desde el punto de vista de la minimizaci√≥n de una funci√≥n de costo. | Esacalabilidad Alta, aunque m√°s lenta que Random Forest, es m√°s precisa. |\n",
        "|XGBoost|Versi√≥n optimizada de Gradient Boosting que incluye **regularizaci√≥n expl√≠cita y paralelizaci√≥n**.|$J = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(f_m)$|Boosting por gradiente de segundo orden con poda (*tree pruning*) y paralelizaci√≥n.  | XGBoost y los esquemas de regresi√≥n de la pregunta 1 persiguen el mismo objetivo fundamental: encontrar una funci√≥n $f(x)$ que aproxime la relaci√≥n entre los datos $x$ y las salidas $t$ minimizando un error esperado. Sin embargo, difieren en la forma de representar esa funci√≥n y en c√≥mo optimizan sus par√°metros. | Escalabilidad Muy alta, dise√±ado para Big Data y GPU computing.|"
      ],
      "metadata": {
        "id": "VzOI9fC7dZU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Librer√≠a RAPIDS y comparaci√≥n de regresores con implementaciones GPU**\n",
        "\n",
        "**¬øQu√© es RAPIDS?**\n",
        "\n",
        "**RAPIDS** es un conjunto de bibliotecas de c√≥digo abierto desarrolladas por **NVIDIA** que permite ejecutar todo el flujo de *Data Science* y *Machine Learning* completamente en **GPU**, sin necesidad de reescribir el c√≥digo tradicional de *scikit-learn* o *pandas*.\n",
        "\n",
        "### Componentes principales de RAPIDS\n",
        "\n",
        "- **cuDF** ‚Üí equivalente a `pandas`, pero procesando *dataframes* en GPU\n",
        "- **cuML** ‚Üí biblioteca de *machine learning* similar a `scikit-learn`, acelerada con CUDA\n",
        "- **cuGraph** ‚Üí algoritmos de grafos en GPU\n",
        "- **cuXfilter**, **cuSpatial** ‚Üí para visualizaci√≥n, geodatos y streaming\n",
        "\n",
        "### Objetivo principal\n",
        "\n",
        "**Acelerar el procesamiento de grandes vol√∫menes de datos** (especialmente cuando `N` o `P` son grandes) mediante c√≥mputo paralelo sobre CUDA. Gracias a que mantiene la **misma interfaz que `scikit-learn`**, es posible migrar c√≥digo CPU ‚Üí GPU con cambios m√≠nimos.\n",
        "\n",
        "### Ventajas principales\n",
        "\n",
        "- Aceleraci√≥n significativa en tareas de regresi√≥n, clasificaci√≥n y clustering\n",
        "- API compatible con `scikit-learn`\n",
        "- Ejecuci√≥n completamente en GPU (sin depender del CPU)\n",
        "\n",
        "### Limitaciones\n",
        "\n",
        "- Algunos algoritmos no est√°n a√∫n disponibles o son parciales (p. ej. `GaussianProcessRegressor`)\n",
        "- Las operaciones con datos peque√±os pueden no mostrar mejora (debido al overhead de transferencia CPU‚ÜîGPU)\n",
        "- Ciertas funciones (como kernels precomputados o datos *sparse*) pueden forzar una ejecuci√≥n en CPU\n",
        "\n",
        "---\n",
        "\n",
        "## C√≥mo migrar un pipeline sklearn ‚Üí RAPIDS (paso a paso, con c√≥digo)\n",
        "\n",
        "**Contexto:** quieres acelerar un pipeline con `pandas` + `sklearn`. Supongamos un regresor `LinearRegression` o `RandomForestRegressor`.\n",
        "\n",
        "### 1) Instalar y comprobar entorno\n",
        "\n",
        "- Instalar RAPIDS compatible con tu CUDA (recomendado: usar conda o RAPIDS con Docker)\n",
        "- Verificar GPU disponible usando `nvidia-smi` y compatibilidad de versiones\n",
        "- Documentaci√≥n: [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "### 2) Convertir pandas ‚Üí cudf\n",
        "\n",
        "Primero importamos las librer√≠as necesarias y convertimos el DataFrame de pandas a cuDF:\n",
        "\n",
        "    import cudf\n",
        "    import pandas as pd\n",
        "    \n",
        "    # df: pandas DataFrame\n",
        "    gdf = cudf.from_pandas(df)        # copia a GPU (memcpy)\n",
        "    # o si lees parquet/csv con cuDF: cudf.read_csv(...)\n",
        "\n",
        "### 3) Entrenar un modelo cuML (ejemplo: LinearRegression)\n",
        "\n",
        "Una vez que los datos est√°n en la GPU, entrenamos el modelo directamente:\n",
        "\n",
        "    from cuml.linear_model import LinearRegression\n",
        "    \n",
        "    X = gdf[feature_cols]   # cudf.DataFrame\n",
        "    y = gdf[target_col]     # cudf.Series\n",
        "    \n",
        "    model = LinearRegression(fit_intercept=True, algorithm='svd')\n",
        "    model.fit(X, y)\n",
        "    preds = model.predict(X_test)   # devuelve cudf.Series o cupy array\n",
        "\n",
        "**Observaciones:** algunos modelos aceptan `cupy`/`numpy` arrays; la salida puede ser `cudf` o `cupy`. Ver [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "### 4) RandomForest (ejemplo)\n",
        "\n",
        "Para modelos basados en √°rboles como Random Forest:\n",
        "\n",
        "    from cuml.ensemble import RandomForestRegressor\n",
        "    \n",
        "    rf = RandomForestRegressor(n_estimators=100, max_depth=16)\n",
        "    rf.fit(X, y)\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "Para ensembles grandes puedes usar Dask-cuML para multi-GPU. Ver [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "\n",
        "### 5) Evaluation (volver a CPU si quieres usar scikit metrics)\n",
        "\n",
        "Si necesitas usar m√©tricas de scikit-learn, puedes transferir los resultados de vuelta a CPU:\n",
        "\n",
        "    import pandas as pd\n",
        "    y_pred_cpu = y_pred.to_pandas()   # transfer back to host if needed\n",
        "\n",
        "**Evita transferencias repetidas:** calcula m√©tricas en GPU cuando sea posible (cuML / cuDF algunas m√©tricas disponibles).\n",
        "\n",
        "---\n",
        "\n",
        "## Multi-GPU y escalado con Dask\n",
        "\n",
        "- **Dask-cuDF:** particiona `cudf` en m√∫ltiples GPUs (o nodos); Dask coordina tareas. Es la forma recomendada para datasets que no caben en la memoria de una sola GPU. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- **cuML + Dask:** muchos estimators tienen versiones `cuml.dask.*` que permiten entrenamiento distribuido (RandomForest, KNN, etc.). La integraci√≥n exige configurar un `Dask` cluster con workers que tengan GPUs asignadas. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "\n",
        "**Consejo:** usar Dask tiene coste de orquestaci√≥n; para muchos problemas una sola GPU potente puede bastar. Para multi-GPU, planifica particionado de datos y comunicaci√≥n (PCIe / NVLink) para minimizar overhead.\n",
        "\n",
        "---\n",
        "\n",
        "## Ventajas y limitaciones pr√°cticas (cuando usar / evitar)\n",
        "\n",
        "### ‚úÖ Ventajas\n",
        "\n",
        "- Aceleraci√≥n significativa en ETL y algunos modelos (RF, Linear, KNN, clustering). [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- API familiar ‚Üí curva de aprendizaje corta. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- Buenas integraciones con XGBoost GPU y Dask para escalado. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "\n",
        "### ‚ö†Ô∏è Limitaciones / riesgos\n",
        "\n",
        "- **Overhead de copia CPU‚ÜîGPU:** para datasets peque√±os la ganancia puede ser nula o negativa. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- **Memoria GPU limitada:** si tus datos no caben, necesitas Dask o streaming. [RAPIDS Docs](https://docs.rapids.ai/)\n",
        "- Algunas implementaciones a√∫n son experimentales y la API evoluciona (revisa la versi√≥n). [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "- No todos los algoritmos avanzados (p. ej. GP completos) est√°n optimizados en cuML; en esos casos usar librer√≠as espec√≠ficas GPU o aproximaciones. (GP ‚Üí ver alternativas)\n",
        "\n",
        "---\n",
        "\n",
        "## Buenas pr√°cticas y checklist antes de migrar a RAPIDS\n",
        "\n",
        "1. **Perfilado:** medir tiempos actuales CPU (ETL + training) para tener baseline\n",
        "2. **Comprobar que los datos caben en GPU** (o dise√±ar particionado)\n",
        "3. **Convertir a cudf al inicio** y hacer el mayor n√∫mero de operaciones en GPU\n",
        "4. **Evitar idas y vueltas:** minimizar `.to_pandas()`/`.to_numpy()`\n",
        "5. **Usar Dask-cuDF** para escalado multi-GPU, y probar con datos reales. [NVIDIA Developer](https://developer.nvidia.com/blog/tag/rapids/)\n",
        "6. **Version pinning:** documentar versi√≥n RAPIDS/CUDA usada ‚Äî cambios en la API son frecuentes\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla comparativa: Regresores de scikit-learn y sus equivalentes en RAPIDS (cuML)\n",
        "\n",
        "| **Modelo / Regresor** | **Scikit-learn** | **Implementaci√≥n en RAPIDS (cuML)** | **Hiperpar√°metros principales** | **Notas / Diferencias** |\n",
        "|------------------------|------------------|-------------------------------------|---------------------------------|--------------------------|\n",
        "| **Linear Regression** | `sklearn.linear_model.LinearRegression` | `cuml.LinearRegression` | `fit_intercept`, `normalize`, m√©todo (`eig`, `svd`, `qr`) | Implementaci√≥n GPU con solvers r√°pidos y paralelizados |\n",
        "| **Ridge Regression (L2)** | `sklearn.linear_model.Ridge` | `cuml.Ridge` | `alpha`, `solver`, `fit_intercept` | Equivalente a MAP con prior gaussiano; usa Eigen o SVD |\n",
        "| **Lasso / ElasticNet** | `sklearn.linear_model.Lasso`, `ElasticNet` | `cuml.ElasticNet` | `alpha`, `l1_ratio`, `max_iter`, `tol` | Lasso puede aproximarse v√≠a `ElasticNet` con `l1_ratio=1.0` |\n",
        "| **SGD Regressor** | `sklearn.linear_model.SGDRegressor` | `cuml.SGD` (experimental) | `loss`, `penalty`, `alpha`, `max_iter`, `learning_rate` | Implementaci√≥n parcial; puede caer a CPU en casos espec√≠ficos |\n",
        "| **Bayesian Ridge** | `sklearn.linear_model.BayesianRidge` | *(no disponible completo en cuML)* | `alpha_1`, `alpha_2`, `lambda_1`, `lambda_2` | No implementado a√∫n con inferencia bayesiana en GPU |\n",
        "| **Kernel Ridge** | `sklearn.kernel_ridge.KernelRidge` | `cuml.KernelRidge` | `alpha`, `kernel`, `gamma`, `degree`, `coef0` | Soporta kernels RBF, linear y poly. No acepta matrices *sparse* |\n",
        "| **Support Vector Regression (SVR)** | `sklearn.svm.SVR` | `cuml.svm.SVR` | `C`, `epsilon`, `kernel`, `gamma`, `degree` | Soporte GPU para kernels comunes; kernels precomputados no |\n",
        "| **Random Forest Regressor** | `sklearn.ensemble.RandomForestRegressor` | `cuml.ensemble.RandomForestRegressor` | `n_estimators`, `max_depth`, `min_samples_split`, `max_features`, `bootstrap` | Totalmente en GPU. Muy superior en datasets grandes |\n",
        "| **Gradient Boosting / XGBoost** | `sklearn.ensemble.GradientBoostingRegressor`, `xgboost.XGBRegressor` | `cuml.GradientBoostingRegressor`, `xgboost` (con GPU) | `n_estimators`, `learning_rate`, `max_depth`, `subsample` | XGBoost GPU es altamente optimizado; integrado con cuDF/cuML |\n",
        "| **Gaussian Process Regressor** | `sklearn.gaussian_process.GaussianProcessRegressor` | *(no implementado en cuML)* | `kernel`, `alpha`, `optimizer`, `normalize_y` | Computacionalmente costoso en GPU; a√∫n sin soporte estable |\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n\n",
        "\n",
        "RAPIDS facilita trasladar los flujos de trabajo cl√°sicos de *scikit-learn* a la GPU manteniendo casi la misma sintaxis. Las bibliotecas m√°s importantes son **cuDF** (para manipular datos) y **cuML** (para ejecutar modelos de regresi√≥n).\n",
        "\n",
        "Los modelos basados en √°rboles (RandomForest, XGBoost) y lineales (Linear, Ridge, ElasticNet, SVR) tienen implementaciones maduras y estables en GPU, ofreciendo **aceleraciones entre 10√ó y 50√ó** en comparaci√≥n con CPU en conjuntos de datos medianos o grandes.\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias bibliogr√°ficas\n",
        "\n",
        "- NVIDIA Corporation. *RAPIDS: Open GPU Data Science Framework*. Disponible en: https://rapids.ai\n",
        "- NVIDIA Developer Blog. *Accelerating Machine Learning with cuML and RAPIDS*. Disponible en: https://developer.nvidia.com/blog/tag/rapids/\n",
        "- RAPIDS cuML Documentation. *cuML API Reference (v25.06)*. Disponible en: https://docs.rapids.ai/api/cuml/stable/\n",
        "- Scikit-Learn Documentation. *Supervised Learning Models*. Disponible en: https://scikit-learn.org/stable/\n",
        "- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer\n",
        "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer"
      ],
      "metadata": {
        "id": "Jp0JSc17dsQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Conjunto de datos nfl-big-data-bowl-2026-prediction**\n",
        "**1. Objetivo Principal del Concurso NFL Big Data Bowl 2026 - Prediction üèà**\n",
        "El objetivo de la competencia \"NFL Big Data Bowl 2026 - Prediction\" es predecir el movimiento de los jugadores en el campo durante una jugada de pase, espec√≠ficamente en los fotogramas que ocurren despu√©s de que el mariscal de campo (quarterback) lanza el bal√≥n.\n",
        "\n",
        "A los participantes se les proporciona una gran cantidad de datos de seguimiento de jugadores (Next Gen Stats) antes del lanzamiento. El desaf√≠o consiste en construir un modelo de regresi√≥n que pueda pronosticar con precisi√≥n las coordenadas (x, y) de cada jugador en el campo para cada fotograma (10 fotogramas por segundo) mientras el bal√≥n est√° en el aire. En esencia, es un problema de predicci√≥n de trayectorias espacio-temporales.\n",
        "\n",
        "**2. Descripci√≥n de las Variables**\n",
        "El conjunto de datos es rico y se compone de varios archivos que deben ser unidos. Las variables clave se pueden agrupar de la siguiente manera:\n",
        "\n",
        "Variables de Entrada (Features)\n",
        "Estas variables describen el estado del juego y de los jugadores antes del lanzamiento del bal√≥n.\n",
        "\n",
        "Variables de Salida (Target)\n",
        "El objetivo es predecir la trayectoria futura de cada jugador.\n",
        "\n",
        "|Variables de entrada|Descripci√≥n|Tipo de variable|Variables de Salida| Descripci√≥n|Tipo de variable|\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "|`game_id`|Identificador del partido|√∫nico (num√©rico)|`game_id`|Identificador del partido|√∫nico (num√©rico)|\n",
        "|`play_id`| Identificador de la jugada|no √∫nico entre juegos (num√©rico)|`play_id`|Identificador de la jugada|no √∫nico entre juegos (num√©rico)|\n",
        "|`player_to_predict`|si la predicci√≥n x/y para este jugador se puntuar√° o no|(booleana)||||\n",
        "|`nfl_id`| Identificador para cada jugador.|√∫nico entre los jugadores (num√©rico)|`nfl_id`| Identificador para cada jugador.|√∫nico entre los jugadores (num√©rico)|\n",
        "|`frame_id`|Identificador de fotograma para cada reproducci√≥n/tipo, comenzando por 1 para cada tipo de `game_id` archivo play_id(entrada o salida). El valor m√°ximo para un determinado game_idser√° `play_id` el `nfl_id` mismo que el num_frames_outputvalor del archivo de entrada correspondiente.|(num√©rico)|`frame_id`|Identificador de fotograma para cada reproducci√≥n/tipo, comenzando por 1 para cada tipo de `game_id` archivo play_id(entrada o salida). El valor m√°ximo para un determinado game_idser√° `play_id` el `nfl_id` mismo que el num_frames_outputvalor del archivo de entrada correspondiente.|(num√©rico)|\n",
        "|`play_direction`|Direcci√≥n en la que se mueve la ofensiva |(izquierda o derecha)||||\n",
        "|`absolute_yardline_number`| Distancia desde la zona de anotaci√≥n para el equipo que posee el bal√≥n|(num√©rica)||||\n",
        "|`player_name`|nombre del jugador | texto ||||\n",
        "|`player_height`|altura del jugador| pies-pulgadas|||||\n",
        "|`player_weight`|peso del jugador| libras||||\n",
        "|`player_birth_date`|fecha de nacimiento|aaaa-mm-dd||||\n",
        "|`player_position`|la posici√≥n del jugador (el rol espec√≠fico en el campo que normalmente desempe√±a)||||\n",
        "|`player_side`|el jugador del equipo est√° en (ataque o defensa)|(ataque o defensa)||||\n",
        "|`player_role`|rol que tiene el jugador en juego| (cobertura defensiva, receptor objetivo, pasador u otro corredor de ruta)||||\n",
        "|`x`|Posici√≥n del jugador a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas.|num√©rico|`x`|Posici√≥n del jugador a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas.|num√©rico|\n",
        "|`y`|Posici√≥n del jugador a lo largo del eje largo del campo, generalmente entre 0 y 53,3 yardas.|num√©rico|`y`|Posici√≥n del jugador a lo largo del eje largo del campo, generalmente entre 0 y 53,3 yardas.|num√©rico|\n",
        "|`s`|Velocidad en yardas/segundo|num√©rica|||||\n",
        "|`a`|Aceleraci√≥n en yardas/segundos^2|num√©rica|||||\n",
        "|`o`|orientaci√≥n del jugador|grados (0-360)|||||\n",
        "|`dir`|angulo de movimiento del jugador|grados (0-360)|||||\n",
        "|`num_frames_output`|numero de fotogramas a predecir en los datos de salida para el `game_id` dado `play_id` y `nfl_id`|num√©rico|||||\n",
        "|`ball_land_x`|Posici√≥n de aterrizaje de la pelota a lo largo del eje largo del campo, generalmente entre 0 y 120 yardas|num√©rico||||"
      ],
      "metadata": {
        "id": "Ogn8cxNud4DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**An√°lisis Exploratorio B√°sico (EDA)**\n",
        "Un buen EDA es fundamental aqu√≠ para entender la din√°mica del juego. Los pasos clave ser√≠an:\n",
        "\n",
        "- Visualizaci√≥n de Jugadas: Crear animaciones de jugadas individuales. Esto es crucial para desarrollar una intuici√≥n sobre los patrones de movimiento. Se puede trazar la posici√≥n (x, y) de cada jugador a lo largo del tiempo, coloreando por equipo y destacando al pasador.\n",
        "\n",
        "- Distribuci√≥n de Variables F√≠sicas: Analizar las distribuciones de velocidad (s), aceleraci√≥n (a), y orientaci√≥n (o) por posici√≥n del jugador (player_position). Se esperar√≠a que los Receptores Abiertos (WR) alcancen velocidades m√°s altas que los Linieros Ofensivos (OL).\n",
        "\n",
        "- An√°lisis de Rutas: Graficar las trayectorias de los receptores para identificar patrones de rutas comunes (ej. \"slant\", \"go\", \"post\").\n",
        "\n",
        "- Correlaciones: Investigar las relaciones entre las variables de entrada. Por ejemplo, la aceleraci√≥n deber√≠a estar correlacionada con los cambios en la velocidad.\n",
        "\n",
        "An√°lisis de Datos Faltantes: Identificar qu√© columnas tienen valores nulos y en qu√© proporci√≥n. Es com√∫n encontrar faltantes en variables contextuales de la jugada.\n",
        "\n",
        "**Codificaci√≥n de Valores Faltantes y Variables Categ√≥ricas**\n",
        "Valores Faltantes:\n",
        "\n",
        "Para variables num√©ricas como o (orientaci√≥n) o dir (direcci√≥n), una estrategia podr√≠a ser la imputaci√≥n con la media o la mediana del jugador o de su grupo de posici√≥n.\n",
        "\n",
        "Otra t√©cnica podr√≠a ser la propagaci√≥n hacia adelante o hacia atr√°s (ffill/bfill) dentro de una misma jugada, asumiendo que el movimiento no cambia dr√°sticamente en un fotograma.\n",
        "\n",
        "**Variables Categ√≥ricas:**\n",
        "\n",
        "player_position, player_role, player_side: Estas son variables categ√≥ricas nominales. La mejor estrategia es el One-Hot Encoding. Esto crea columnas binarias para cada categor√≠a, evitando que el modelo asuma un orden inexistente.\n",
        "\n",
        "player_height: Esta variable viene como texto (ej. \"6-2\"). Se debe convertir a una variable num√©rica (en pulgadas o cent√≠metros) antes de usarla.\n",
        "\n",
        "**Estrategias de Ingenier√≠a de Caracter√≠sticas (Feature Engineering)**\n",
        "Aqu√≠ es donde se puede obtener la mayor ventaja competitiva. El objetivo es crear variables que capturen la compleja interacci√≥n entre jugadores. üß†\n",
        "\n",
        "- Estandarizaci√≥n del Campo: Las jugadas ocurren en diferentes direcciones. Un paso crucial es estandarizar la direcci√≥n de la jugada para que la ofensiva siempre ataque hacia el mismo lado del campo. Esto hace que los patrones sean comparables entre jugadas.\n",
        "\n",
        "- Caracter√≠sticas Relativas al Bal√≥n: Calcular la distancia y el √°ngulo de cada jugador con respecto al bal√≥n en cada fotograma.\n",
        "\n",
        "- Caracter√≠sticas Inter-Jugadores:\n",
        "\n",
        "- Distancia al Jugador M√°s Cercano: Para cada jugador, calcular la distancia a su defensor m√°s cercano (si es ofensivo) o al jugador que est√° cubriendo (si es defensivo).\n",
        "\n",
        "- Distancia al Quarterback: Especialmente relevante para los pass rushers y la l√≠nea ofensiva que los bloquea.\n",
        "\n",
        "- Caracter√≠sticas Cinem√°ticas Derivadas:\n",
        "\n",
        "- Componentes de Velocidad: Descomponer la velocidad s en sus componentes vx y vy usando la direcci√≥n dir. Esto es m√°s √∫til para el modelo que la velocidad escalar.\n",
        "\n",
        "vx = s * cos(dir)\n",
        "\n",
        "vy = s * sin(dir)\n",
        "\n",
        "- Aceleraci√≥n Angular: Calcular la tasa de cambio de la orientaci√≥n o para medir qu√© tan r√°pido est√° girando un jugador.\n",
        "\n",
        "**Caracter√≠sticas de Series Temporales:**\n",
        "\n",
        "Para cada jugador, crear caracter√≠sticas que resuman su movimiento en los √∫ltimos k fotogramas, como velocidad promedio, aceleraci√≥n m√°xima, o cambio total en la direcci√≥n.\n",
        "\n",
        "Al implementar estas estrategias, el conjunto de datos se enriquece enormemente, pasando de ser una simple colecci√≥n de coordenadas a un sistema que describe las complejas relaciones e interacciones en el campo de juego. Esto es fundamental para que los modelos de regresi√≥n puedan aprender los patrones de movimiento de manera efectiva."
      ],
      "metadata": {
        "id": "ugQuvZxUePSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%markdown\n",
        "\n",
        "# M√©tricas de Evaluaci√≥n de Modelos de Regresi√≥n\n",
        "\n",
        "## Notaci√≥n\n",
        "\n",
        "- **y·µ¢**: El valor real u observado para la i-√©sima observaci√≥n.\n",
        "- **≈∑·µ¢**: El valor predicho por el modelo para la i-√©sima observaci√≥n.\n",
        "- **N**: El n√∫mero total de observaciones en el conjunto de datos.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. MAE (Mean Absolute Error / Error Absoluto Medio)\n",
        "\n",
        "El MAE es quiz√°s la m√©trica de error m√°s intuitiva y f√°cil de interpretar.\n",
        "\n",
        "### ¬øQu√© Mide?\n",
        "Mide el promedio de las diferencias absolutas entre los valores reales y los predichos. En palabras simples, te dice: \"en promedio, ¬øpor cu√°ntas unidades me equivoqu√©, sin importar si fue por arriba o por abajo?\"\n",
        "\n",
        "### C√°lculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error**: Para cada punto, calcula la diferencia entre el valor real y el predicho (y·µ¢ - ≈∑·µ¢).\n",
        "2. **Obtener el Valor Absoluto**: Convierte cada error a un valor positivo tomando su valor absoluto (|y·µ¢ - ≈∑·µ¢|). Esto elimina la direcci√≥n del error.\n",
        "3. **Sumar los Errores**: Suma todos los errores absolutos que calculaste.\n",
        "4. **Calcular el Promedio**: Divide la suma total por el n√∫mero de observaciones (N).\n",
        "\n",
        "### F√≥rmula Matem√°tica:\n",
        "\n",
        "$$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
        "\n",
        "### Interpretaci√≥n:\n",
        "\n",
        "- **Unidades**: El resultado est√° en las mismas unidades que tu variable objetivo. Si est√°s prediciendo yardas, el MAE estar√° en yardas.\n",
        "- **Escala**: Un valor de 0 es un modelo perfecto. Valores m√°s altos indican un mayor error promedio.\n",
        "- **Fortaleza**: Es menos sensible a los valores at√≠picos (outliers) que el MSE, ya que no eleva los errores al cuadrado.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. MSE (Mean Squared Error / Error Cuadr√°tico Medio)\n",
        "\n",
        "El MSE es una de las m√©tricas m√°s comunes, especialmente en la optimizaci√≥n de modelos.\n",
        "\n",
        "### ¬øQu√© Mide?\n",
        "Mide el promedio de los errores al cuadrado. Su principal caracter√≠stica es que penaliza los errores grandes mucho m√°s que los errores peque√±os. Un error de 10 unidades contribuye 100 veces m√°s al error total que un error de 1 unidad.\n",
        "\n",
        "### C√°lculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error**: Para cada punto, encuentra la diferencia (y·µ¢ - ≈∑·µ¢).\n",
        "2. **Elevar al Cuadrado**: Eleva cada error al cuadrado ((y·µ¢ - ≈∑·µ¢)¬≤). Esto convierte todos los errores en positivos y magnifica los m√°s grandes.\n",
        "3. **Sumar los Errores**: Suma todos los errores cuadr√°ticos.\n",
        "4. **Calcular el Promedio**: Divide la suma total por el n√∫mero de observaciones (N).\n",
        "\n",
        "### F√≥rmula Matem√°tica:\n",
        "\n",
        "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "### Interpretaci√≥n:\n",
        "\n",
        "- **Unidades**: Las unidades est√°n al cuadrado con respecto a la variable objetivo (ej. yardas¬≤), lo que hace que su interpretaci√≥n directa sea menos intuitiva. Por esta raz√≥n, a menudo se usa su ra√≠z cuadrada, el RMSE (Root Mean Squared Error), que vuelve a las unidades originales.\n",
        "- **Sensibilidad**: Es muy sensible a los outliers. Un solo punto muy mal predicho puede disparar el valor del MSE.\n",
        "- **Uso**: Es muy popular porque la funci√≥n de error cuadr√°tico es convexa y diferenciable, lo que facilita los c√°lculos matem√°ticos en los algoritmos de optimizaci√≥n (como el descenso de gradiente).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. R¬≤ (Coefficient of Determination / Coeficiente de Determinaci√≥n)\n",
        "\n",
        "R¬≤ no es una medida de error, sino una medida de qu√© tan bien se ajusta el modelo a los datos.\n",
        "\n",
        "### ¬øQu√© Mide?\n",
        "Mide la proporci√≥n de la varianza en la variable objetivo que es explicada por el modelo. Responde a la pregunta: \"¬øqu√© porcentaje de la variabilidad de los datos es capturado por mi modelo, en comparaci√≥n con un modelo muy simple que solo predice el promedio de los datos?\"\n",
        "\n",
        "### C√°lculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error del Modelo (RSS)**: Calcula la suma de los errores al cuadrado de tu modelo. Esto es N√óMSE. Se conoce como Suma de Cuadrados Residual (RSS).\n",
        "   \n",
        "   $$RSS = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "2. **Calcular el Error de un Modelo \"Base\" (TSS)**: Calcula la suma de los errores al cuadrado que obtendr√≠as si tu predicci√≥n fuera siempre el promedio de todos los valores reales (»≥). Esto representa la varianza total de los datos y se conoce como Suma de Cuadrados Total (TSS).\n",
        "   \n",
        "   $$TSS = \\sum_{i=1}^{N} (y_i - \\bar{y})^2$$\n",
        "\n",
        "3. **Calcular R¬≤**: Compara el error de tu modelo con el error del modelo base.\n",
        "   \n",
        "   $$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
        "\n",
        "### F√≥rmula Matem√°tica:\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}$$\n",
        "\n",
        "### Interpretaci√≥n:\n",
        "\n",
        "- **Escala**: No tiene unidades. Su valor suele estar entre 0 y 1.\n",
        "- **R¬≤ = 1**: El modelo explica el 100% de la variabilidad de los datos. Es un ajuste perfecto.\n",
        "- **R¬≤ = 0**: El modelo no explica nada de la variabilidad. Es tan √∫til como simplemente predecir el promedio.\n",
        "- **R¬≤ < 0**: El modelo es peor que predecir el promedio. Esto indica un muy mal ajuste.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. MAPE (Mean Absolute Percentage Error / Error Porcentual Absoluto Medio)\n",
        "\n",
        "MAPE es muy popular en contextos de negocio porque expresa el error en t√©rminos de un porcentaje f√°cil de entender.\n",
        "\n",
        "### ¬øQu√© Mide?\n",
        "Mide el promedio del error absoluto, expresado como un porcentaje del valor real. Te dice: \"en promedio, ¬øqu√© tan lejos estuvieron mis predicciones de los valores reales, en t√©rminos porcentuales?\"\n",
        "\n",
        "### C√°lculo Paso a Paso:\n",
        "\n",
        "1. **Calcular el Error Porcentual**: Para cada punto, calcula el error absoluto y div√≠delo por el valor real absoluto: $\\frac{|y_i - \\hat{y}_i|}{|y_i|}$.\n",
        "2. **Sumar los Errores Porcentuales**: Suma todos los errores porcentuales que calculaste.\n",
        "3. **Calcular el Promedio**: Divide la suma total por el n√∫mero de observaciones (N).\n",
        "4. **Expresar como Porcentaje**: Multiplica el resultado por 100.\n",
        "\n",
        "### F√≥rmula Matem√°tica:\n",
        "\n",
        "$$MAPE = \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - \\hat{y}_i|}{|y_i|}\\right) \\times 100\\%$$\n",
        "\n",
        "### Interpretaci√≥n:\n",
        "\n",
        "- **Unidades**: Es un porcentaje (%). Un MAPE del 5% significa que, en promedio, el error de predicci√≥n es del 5% del valor real.\n",
        "- **Fortaleza**: Es muy f√°cil de interpretar y comunicar a audiencias no t√©cnicas.\n",
        "- **Debilidad Cr√≠tica**: No se puede usar si los valores reales (y·µ¢) son cero o muy cercanos a cero, ya que la divisi√≥n por cero es indefinida o resultar√≠a en valores enormes, distorsionando el resultado final."
      ],
      "metadata": {
        "id": "4PSE2d8yeS-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Link desarrollo del parcial  - KAGGLE Base de datos NFL**\n",
        "lINK:https://www.kaggle.com/code/disalazarp/tam-parcial-1"
      ],
      "metadata": {
        "id": "vSLIT0sLemjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NZjyacnMeGFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZZZoG7DctPx"
      },
      "outputs": [],
      "source": [
        "Integrantes: Valeria Garcia - Diego Salazar - Julian David Morales"
      ]
    }
  ]
}